# -*- coding: utf-8 -*-
"""Text-to-Date Format Conversion Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14rCypD5T7hnBLRfjDiLPKPAshKQESpnj

# Task1: Dataset Generation

**Install Libraries and Packages**
"""

!pip install persian-tools

!pip install jdatetime

import random
from persian_tools import digits
import jdatetime
import pandas as pd
import itertools

"""**Helper Functions**"""

def convert_month_number_to_name(number):
  persian_months = {
      1: "فروردین",
      2: "اردیبهشت",
      3: "خرداد",
      4: "تیر",
      5: "مرداد",
      6: "شهریور",
      7: "مهر",
      8: "آبان",
      9: "آذر",
      10: "دی",
      11: "بهمن",
      12: "اسفند"
  }

  return persian_months.get(number, "")

persian_digits = {
    '0': '۰',
    '1': '۱',
    '2': '۲',
    '3': '۳',
    '4': '۴',
    '5': '۵',
    '6': '۶',
    '7': '۷',
    '8': '۸',
    '9': '۹'
}

def convert_to_persian_digits(number_str):
    return ''.join(persian_digits.get(digit, digit) for digit in number_str)

def add_m_suffix(word):
    if word.endswith('ه'):
        return word[:-1] + 'هم'
    else:
        return word + 'م'

def get_day_representations(day):
    day_standard = str(day)
    day_persian = convert_to_persian_digits(day_standard)
    day_word = digits.convert_to_word(day)
    day_word_m = add_m_suffix(day_word)
    day_with_ruz = [f"روز {day_standard}", f"روز {day_persian}", f"روز {day_word}"]
    return [day_standard, day_persian, day_word, day_word_m] + day_with_ruz

def get_month_representations(month):
    month_standard = str(month)
    month_persian = convert_to_persian_digits(month_standard)
    month_word = digits.convert_to_word(month)
    month_word_m = add_m_suffix(month_word)
    month_name = convert_month_number_to_name(month)
    month_with_mah = [f"ماه {month_standard}", f"ماه {month_persian}", f"ماه {month_word}", f"ماه {month_name}"]
    return [month_standard, month_persian, month_word, month_word_m, month_name] + month_with_mah

def get_year_representations(year):
    year_standard = str(year)
    year_persian = convert_to_persian_digits(year_standard)
    year_word = digits.convert_to_word(year)
    year_with_sal = [f"سال {year_standard}", f"سال {year_persian}", f"سال {year_word}"]
    return [year_standard, year_persian, year_word] + year_with_sal

"""**Create Dataset**"""

def generate_dataset(date):
    day = date['day']
    month = date['month']
    year = date['year']
    formal_date = f"{year}/{month:02d}/{day:02d}"

    day_reps = get_day_representations(day)
    month_reps = get_month_representations(month)
    year_reps = get_year_representations(year)

    separators = ['/', '.', '-', ' ']


    patterns = [
        "{day} {month} {year}",
        "{day} {month} {year_with_sal}",
        "در {day} {month} {year}",
        "به تاریخ {day} {month} {year}",
        "{day} {month} سال {year}",
        "{day_with_ruz} {month} {year}",
        "{day} {month_with_mah} {year}",
        "{day} {month} {year_persian}",
        "{day_persian} {month_persian} {year_persian}",
        # New patterns with order changes
        "{year} {month} {day}",
        "{year} {day} {month}",
        "{month} {day} {year}",
        "{day} {year} {month}",
        "{month} {year} {day}",
        # Numeric formats with separators
        "{year_num}{sep}{month_num}{sep}{day_num}",
        "{day_num}{sep}{month_num}{sep}{year_num}",
        "{month_num}{sep}{day_num}{sep}{year_num}",
        "{year_num}{sep}{day_num}{sep}{month_num}",
        "{day_num}{sep}{year_num}{sep}{month_num}",
        "{month_num}{sep}{year_num}{sep}{day_num}",
    ]




    data = []
    for pattern in patterns:
        for day_rep in day_reps:
            for month_rep in month_reps:
                for year_rep in year_reps:
                    if 'num' in pattern:
                        for sep in separators:
                            informal_text = pattern.format(
                                day_num=str(day),
                                month_num=str(month),
                                year_num=str(year),
                                sep=sep
                            ).strip()
                            data.append({'informal_text': informal_text, 'formal_date': formal_date})
                            informal_text_persian = pattern.format(
                                day_num=convert_to_persian_digits(str(day)),
                                month_num=convert_to_persian_digits(str(month)),
                                year_num=convert_to_persian_digits(str(year)),
                                sep=sep
                            ).strip()
                            data.append({'informal_text': informal_text_persian, 'formal_date': formal_date})
                    else:
                        informal_text = pattern.format(
                            day=day_rep,
                            day_with_ruz=day_rep,
                            day_persian=convert_to_persian_digits(day_rep),
                            month=month_rep,
                            month_with_mah=month_rep,
                            month_persian=convert_to_persian_digits(month_rep),
                            year=year_rep,
                            year_with_sal=year_rep,
                            year_persian=convert_to_persian_digits(year_rep),
                            year_num=str(year),
                            month_num=str(month),
                            day_num=str(day),
                        ).strip()
                        data.append({'informal_text': informal_text, 'formal_date': formal_date})

    return data

dates_list = [
    {'day': 1 , 'month': 1, 'year': 1370},
    # {'day': 15, 'month': 7, 'year': 1380},
    # {'day': 29, 'month': 12, 'year': 1380},
    # {'day': 30, 'month': 12, 'year': 1390},
    # {'day': 22, 'month': 11, 'year': 1390},
    # {'day': 13, 'month': 1, 'year': 1396},
    # {'day': 10, 'month': 2, 'year': 1400},
    # {'day': 30, 'month': 12, 'year': 1403},
    # {'day': 22 ,'month': 3, 'year': 1410},
    # {'day': 17, 'month': 5, 'year': 1415},
]

dataset = []

for date_info in dates_list:
  dataset.extend(generate_dataset(date_info))


=df = pd.DataFrame(dataset)
df.drop_duplicates(inplace=True)
df.to_csv('dataset.csv', index=False, encoding='utf-8-sig')

print(f"Generated {len(df)} entries in the dataset.")

"""# Task 2: Model Development

**Install Libraries and Packages**
"""

!pip install hazm

"""**Preprocessing**"""

from hazm import Normalizer

normalizer = Normalizer(persian_numbers=False)

def normalize_text(text):
    normalized_text = normalizer.normalize(text)
    return normalized_text

df['informal_text_normalized'] = df['informal_text'].apply(normalize_text)
df['formal_date_normalized'] = df['formal_date'].apply(normalize_text)

df.to_csv('dataset_normalized.csv', index=False, encoding='utf-8-sig')

from transformers import T5Tokenizer
from datasets import Dataset

model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)

!pip install transformers datasets

def preprocess_data(examples, tokenizer, max_length=128):
    inputs = examples['informal_text_normalized']
    targets = examples['formal_date_normalized']

    # Tokenize input and target sequences
    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length')

    # Add the labels to the input dictionary
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

dataset = Dataset.from_pandas(df)

preprocessed_dataset = dataset.map(lambda x: preprocess_data(x, tokenizer), batched=True)

train_test_split = preprocessed_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

"""**Function to compute metrics**"""

from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    accuracy = accuracy_score(decoded_labels, decoded_preds)
    return {"accuracy": accuracy}

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True
)

from transformers import T5ForConditionalGeneration, Trainer

model = T5ForConditionalGeneration.from_pretrained("t5-small")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics  # Add the compute_metrics function
)

trainer.train()

model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")